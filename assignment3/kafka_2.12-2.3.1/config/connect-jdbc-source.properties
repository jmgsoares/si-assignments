# Name of the JDBC Source
name=jdbc-source-postgresql
# Class of the JDBC connector
connector.class=io.confluent.connect.jdbc.JdbcSourceConnector
# String to connect to the DB
connection.url=jdbc:postgresql://localhost:5432/kafkaDb
# DB User Name
connection.user=KafkaDbUs3r
# DB User Password
connection.password=KafkaDbUs3rPwD
# DB Dialect
dialect.name=PostgreSqlDatabaseDialect

# Mode - The mode for updating a table each time it is polled.
# bulk: perform a bulk load of the entire table each time it is polled
# timestamp: use a timestamp (or timestamp-like) column to detect new and modified rows. This assumes the column is updated with each write, and that values are monotonically incrementing, but not necessarily unique
# timestamp+incrementing: use two columns, a timestamp column that detects new and modified rows and a strictly incrementing column which provides a globally unique ID for updates so each row can be assigned a unique stream offset
mode=timestamp+incrementing

incrementing.column.name=id
timestamp.column.name=updated

# Query to run agains the DB (If we use this the topic.prefix becomes the topic name to use)
#query=SELECT type, id, name FROM data;

# Poll interval
poll.interval.ms=60000

# Topic where to save the data
topic.prefix=db

# Transformations to apply
transforms=CreateKey, ExtractKey, FilterValue
transforms.CreateKey.type=org.apache.kafka.connect.transforms.ValueToKey
transforms.CreateKey.fields=type
transforms.ExtractKey.type=org.apache.kafka.connect.transforms.ExtractField$Key
transforms.ExtractKey.field=type
transforms.FilterValue.type=org.apache.kafka.connect.transforms.ReplaceField$Value
transforms.FilterValue.whitelist=id,name

# Disable the schemas, for we don't need the extra verbose information
key.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=false
value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=false


